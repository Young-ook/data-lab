{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc244095-d9d7-47e9-8db3-756ed320513c",
   "metadata": {},
   "source": [
    "# PySpark (Local)\n",
    "## What is PySpark\n",
    "PySpark is the Python API for [Apache Spark](https://spark.apache.org/). PySpark enables developers to write Spark applications using Python, providing access to Spark’s rich set of features and capabilities through Python language. With its rich set of features, robust performance, and extensive ecosystem, PySpark has become a popular choice for data engineers, data scientists, and developers working with big data and distributed computing. PySpark is very well used in the Data Science and Machine Learning community as there are many widely used data science libraries written in Python including NumPy, and TensorFlow. Also used due to its efficient processing of large datasets.\n",
    "\n",
    "Spark has a multi-language engine, that provides APIs (Application Programming Interfaces) and libraries for several programming languages like Java, Scala, Python, and R, allowing developers to work with Spark using the language they are most comfortable with.\n",
    "\n",
    "- Scala: Spark’s primary and native language is Scala. Many of Spark’s core components are written in Scala, and it provides the most extensive API for Spark.\n",
    "- Java: Spark provides a Java API that allows developers to use Spark within Java applications. Java developers can access most of Spark’s functionality through this API.\n",
    "- R: Spark also offers an R API, enabling R users to work with Spark data and perform distributed data analysis using their familiar R language.\n",
    "- Python: Spark offers a Python API, called PySpark, which is popular among data scientists and developers who prefer Python for data analysis and machine learning tasks. PySpark provides a Pythonic way to interact with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e113295",
   "metadata": {},
   "source": [
    "## Iinitialize PySpark Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9f491-9c27-42d9-bc20-5e14057ac9e1",
   "metadata": {},
   "source": [
    "This example requires spark environment. Please make sure that you've installed spark application on your environment before you begin. This is simple script to verify if a spark application is installed and configured.\n",
    "```\n",
    "# It is required to set the SPARK_HOME environment variable.\n",
    "# Please make sure the variable indicates to the right path to your spark.\n",
    "if [ -z $SPARK_HOME ] ; then\n",
    "  export SPARK_HOME=\"$HOME/.local/lib/spark-3.5.4-bin-hadoop3\"\n",
    "fi\n",
    "```\n",
    "\n",
    "For more details, please refer to the [Install Spark](https://github.com/Young-ook/data-lab-on-wsl?tab=readme-ov-file#install-spark) and follow the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9c2650-f597-4961-ac05-56bc7883f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark                 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# validate findspark\n",
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3892f8-aef8-4c90-9654-b99ae947e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c7516-5dae-493c-9c36-caaa0a48ece9",
   "metadata": {},
   "source": [
    "## PySpark on Local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4d5f02-08b2-4f00-b0ea-8a317a6e650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/29 15:26:37 WARN Utils: Your hostname, emma resolves to a loopback address: 127.0.1.1; using 172.18.245.201 instead (on interface eth0)\n",
      "25/01/29 15:26:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/29 15:26:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.245.201:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcda03f06d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a new spark context\n",
    "#sc = pyspark.SparkContext(master=\"local\", appName=\"pyspark-basic\")\n",
    "#spark = SparkSession(sc)\n",
    "#\n",
    "# or\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"pyspark-basic\").getOrCreate()\n",
    "\n",
    "# display spark session (local master)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e23d80-5b46-4908-8d91-024decea829f",
   "metadata": {},
   "source": [
    "## DataFrame example\n",
    "### What is DataFrame\n",
    "A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset\\[Row\\]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b0ac0-f570-4d68-a8fa-43e105c2ca74",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d240461-c4bf-4bcb-ab7b-24fcf822a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the log level\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ff5d65-6d2d-415c-8003-58d816a219ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cd: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- div: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+---+------+\n",
      "| cd|    name|gender|age|div|salary|\n",
      "+---+--------+------+---+---+------+\n",
      "|001|   Smith|     M| 40| DA|  4000|\n",
      "|002|    Rose|     M| 35| DA|  3000|\n",
      "|003|Williams|     M| 30| DE|  2500|\n",
      "|004|    Anne|     F| 30| DE|  3000|\n",
      "|005|    Mary|     F| 35| BE|  4000|\n",
      "|006|   James|     M| 30| FE|  3500|\n",
      "+---+--------+------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame\n",
    "data = [('001','Smith','M',40,'DA',4000),\n",
    "        ('002','Rose','M',35,'DA',3000),\n",
    "        ('003','Williams','M',30,'DE',2500),\n",
    "        ('004','Anne','F',30,'DE',3000),\n",
    "        ('005','Mary','F',35,'BE',4000),\n",
    "        ('006','James','M',30,'FE',3500)]\n",
    "\n",
    "columns = [\"cd\",\"name\",\"gender\",\"age\",\"div\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83e83e-0aa0-4f51-b537-4ff13bfb53ef",
   "metadata": {},
   "source": [
    "### Load from file\n",
    "By utilizing `DataFrameReader.csv(\"path\")` or `format(\"csv\").load(\"path\")` methods, you can read a CSV file into a PySpark DataFrame. These methods accept a file path as their parameter. When using the format(“csv”) approach, you should specify data sources like *csv* or *org.apache.spark.sql.csv*.\n",
    "\n",
    "Download the zipcode.csv file form the spark examples [repository](https://github.com/spark-examples/pyspark-examples/blob/master/resources/zipcodes.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85d837c-4824-4e5a-a27f-c1c2c9e226f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3035  100  3035    0     0  57410      0 --:--:-- --:--:-- --:--:-- 58365\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://raw.githubusercontent.com/spark-examples/pyspark-examples/refs/heads/master/resources/zipcodes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6064c211-63a1-4e2d-b569-8f138d65aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV File\n",
    "df = spark.read.csv(\"./zipcodes.csv\")\n",
    "# or\n",
    "# alternatively, you can use the format().load()\n",
    "#df = spark.read.format(\"csv\").load(\"./zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683d125-769b-480f-84d6-59b8f62bbec0",
   "metadata": {},
   "source": [
    "If you have a header with column names on your input file, you need to explicitly specify True for header option using option(\"header\",True) not mentioning this, the API treats header as a data record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f32925f-56df-412e-809e-ab97a2e16af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|  Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           NULL|               NULL|      NULL| NULL|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96|-66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           NULL|               NULL|      NULL| NULL|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14|-66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL| NULL|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72|-97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           NULL|               NULL|      NULL| NULL|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75|-97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986| NULL|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # use header record for column names\n",
    "df_wh = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(\"./zipcodes.csv\")\n",
    "df_wh.head()\n",
    "df_wh.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c596fc4-9741-441e-a373-a30101f9b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the current spark session before connecting to standalone cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d14ca",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "- [Apache Spark Examples](https://spark.apache.org/examples.html)\n",
    "- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856baad4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
