{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc244095-d9d7-47e9-8db3-756ed320513c",
   "metadata": {},
   "source": [
    "# PySpark Tutorial\n",
    "## What is PySpark\n",
    "PySpark is the Python API for [Apache Spark](https://spark.apache.org/). PySpark enables developers to write Spark applications using Python, providing access to Spark’s rich set of features and capabilities through Python language. With its rich set of features, robust performance, and extensive ecosystem, PySpark has become a popular choice for data engineers, data scientists, and developers working with big data and distributed computing. PySpark is very well used in the Data Science and Machine Learning community as there are many widely used data science libraries written in Python including NumPy, and TensorFlow. Also used due to its efficient processing of large datasets.\n",
    "\n",
    "Spark has a multi-language engine, that provides APIs (Application Programming Interfaces) and libraries for several programming languages like Java, Scala, Python, and R, allowing developers to work with Spark using the language they are most comfortable with.\n",
    "\n",
    "- Scala: Spark’s primary and native language is Scala. Many of Spark’s core components are written in Scala, and it provides the most extensive API for Spark.\n",
    "- Java: Spark provides a Java API that allows developers to use Spark within Java applications. Java developers can access most of Spark’s functionality through this API.\n",
    "- R: Spark also offers an R API, enabling R users to work with Spark data and perform distributed data analysis using their familiar R language.\n",
    "- Python: Spark offers a Python API, called PySpark, which is popular among data scientists and developers who prefer Python for data analysis and machine learning tasks. PySpark provides a Pythonic way to interact with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e113295",
   "metadata": {},
   "source": [
    "## Iinitialize PySpark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9c2650-f597-4961-ac05-56bc7883f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark                 2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3892f8-aef8-4c90-9654-b99ae947e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c7516-5dae-493c-9c36-caaa0a48ece9",
   "metadata": {},
   "source": [
    "## PySpark on Local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4d5f02-08b2-4f00-b0ea-8a317a6e650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.23:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5e9026ae10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## create a new spark context\n",
    "#sc = pyspark.SparkContext(master=\"local\", appName=\"pyspark-basic\")\n",
    "#spark = SparkSession(sc)\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"pyspark-basic\").getOrCreate()\n",
    "\n",
    "# display spark session (local master)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e23d80-5b46-4908-8d91-024decea829f",
   "metadata": {},
   "source": [
    "## DataFrame example\n",
    "### What is DataFrame\n",
    "A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.\n",
    "\n",
    "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset\\[Row\\]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b0ac0-f570-4d68-a8fa-43e105c2ca74",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d240461-c4bf-4bcb-ab7b-24fcf822a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the log level\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8ff5d65-6d2d-415c-8003-58d816a219ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cd: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- div: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+---+------+\n",
      "| cd|    name|gender|age|div|salary|\n",
      "+---+--------+------+---+---+------+\n",
      "|001|   Smith|     M| 40| DA|  4000|\n",
      "|002|    Rose|     M| 35| DA|  3000|\n",
      "|003|Williams|     M| 30| DE|  2500|\n",
      "|004|    Anne|     F| 30| DE|  3000|\n",
      "|005|    Mary|     F| 35| BE|  4000|\n",
      "|006|   James|     M| 30| FE|  3500|\n",
      "+---+--------+------+---+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# create DataFrame\n",
    "data = [('001','Smith','M',40,'DA',4000),\n",
    "        ('002','Rose','M',35,'DA',3000),\n",
    "        ('003','Williams','M',30,'DE',2500),\n",
    "        ('004','Anne','F',30,'DE',3000),\n",
    "        ('005','Mary','F',35,'BE',4000),\n",
    "        ('006','James','M',30,'FE',3500)]\n",
    "\n",
    "columns = [\"cd\",\"name\",\"gender\",\"age\",\"div\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83e83e-0aa0-4f51-b537-4ff13bfb53ef",
   "metadata": {},
   "source": [
    "### Load from file\n",
    "By utilizing `DataFrameReader.csv(\"path\")` or `format(\"csv\").load(\"path\")` methods, you can read a CSV file into a PySpark DataFrame. These methods accept a file path as their parameter. When using the format(“csv”) approach, you should specify data sources like *csv* or *org.apache.spark.sql.csv*.\n",
    "\n",
    "Download the zipcode.csv file form the spark examples [repository](https://github.com/spark-examples/pyspark-examples/blob/master/resources/zipcodes.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a85d837c-4824-4e5a-a27f-c1c2c9e226f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239k    0  239k    0     0   384k      0 --:--:-- --:--:-- --:--:--  384k\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://github.com/spark-examples/pyspark-examples/blob/master/resources/zipcodes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6064c211-63a1-4e2d-b569-8f138d65aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read CSV File\n",
    "df = spark.read.csv(\"./zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77c76039-4e88-4c6e-9045-4fd56ab4a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the current spark session before connecting to standalone cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3884da-2c2e-4d11-9a35-c922d47785de",
   "metadata": {},
   "source": [
    "## PySpark on Stndalone Cluster\n",
    "Before you begin, you need to make sure that your standalone spark cluster is running. If your cluster is not running, please follow the instructions to [run a spark standalone cluster](https://github.com/Young-ook/data-lab-on-wsl?tab=readme-ov-file#launch-a-standalone-cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e961bba0-fe4a-4185-8beb-a13520d9bcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.23:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://localhost:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(master=\"spark://localhost:7077\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca34ff-6693-452e-a7ab-62deb97f5c4f",
   "metadata": {},
   "source": [
    "## Pi example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c1c93f-2d23-484c-9da5-6b6714155135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1416436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15ce6fd9-48af-4c3e-9337-36444ee9c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the current spark session for cleanup\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a8cf2-d78e-4a5b-a499-50e11df9c1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333d14ca",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "- [Apache Spark Examples](https://spark.apache.org/examples.html)\n",
    "- [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856baad4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
